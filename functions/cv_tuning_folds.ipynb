{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f965c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore') # prob not best practice (fix later)\n",
    "\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from skopt import BayesSearchCV\n",
    "from joblib import parallel_backend\n",
    "\n",
    "from duqling_interface import DuqlingInterface\n",
    "from model_search_spaces import get_models\n",
    "from plot_performance import plot_bayes_cv_rmse, heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4673ce6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "duq = DuqlingInterface()\n",
    "\n",
    "univariate_funcs = duq.list_functions(response_type='uni').fname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8389eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_performance(bayes_search, X_test:np.array, y_test:np.array):\n",
    "    \"\"\"\n",
    "    Compute the test RMSE, standard deviation (sigma), and Pearson's R value\n",
    "    and record them in the cross validation results dictionary.\n",
    "    \"\"\"\n",
    "    y_pred = bayes_search.best_estimator_.predict(X_test)\n",
    "    rmse  = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    sigma = y_test.std(ddof=0)\n",
    "    r     = np.corrcoef(y_pred, y_test)[0, 1]\n",
    "    bayes_search.cv_results_['test_rmse'] = rmse\n",
    "    bayes_search.cv_results_['sigma']     = sigma\n",
    "    bayes_search.cv_results_['r_val']     = r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a7cff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_cv_results(cv_results:dict, savepath:Path):\n",
    "    savepath.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(savepath, 'wb') as f:\n",
    "        pickle.dump(cv_results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01146a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "for duqling_func_name in univariate_funcs[3:5]:\n",
    "    X, y = duq.generate_data(duqling_func_name, n_samples=100, seed=42)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.25, random_state=42\n",
    "    )\n",
    "    n_features = X.shape[1]\n",
    "    models = get_models(n_features)\n",
    "    for model_name in models:\n",
    "\n",
    "        # Configure the tqdm progress bar\n",
    "        total_iters = models[model_name]['n_iter']\n",
    "        bar = tqdm(total=total_iters, desc=f\"{model_name.upper()} on {duqling_func_name}\", unit=\"iter\")\n",
    "        def update_tqdm_bar(_): bar.update()\n",
    "\n",
    "        bayes_search = BayesSearchCV(\n",
    "            **models[model_name],\n",
    "            cv=3,\n",
    "            scoring='neg_mean_squared_error',\n",
    "            random_state=42,\n",
    "            verbose=0,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        with parallel_backend('threading', n_jobs=-1):\n",
    "            bayes_search.fit(X_train, y_train, callback=update_tqdm_bar)\n",
    "        bar.close()\n",
    "        \n",
    "        record_performance(bayes_search, X_test, y_test)\n",
    "\n",
    "        plot_bayes_cv_rmse(bayes_search.cv_results_, model_name.upper(), duqling_func_name)\n",
    "\n",
    "        savepath = Path(\"models\", model_name, duqling_func_name, f\"cv_results.pkl\")\n",
    "        save_cv_results(bayes_search.cv_results_, savepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce11ba27",
   "metadata": {},
   "source": [
    "## Visualize Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e2f2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "\n",
    "metrics     = [\"test_rmse\", \"sigma\", \"r_val\"]\n",
    "model_names = list(get_models(1).keys())\n",
    "\n",
    "arr = np.full((len(metrics), len(model_names), len(univariate_funcs)), np.nan)\n",
    "\n",
    "for j, model in enumerate(model_names):\n",
    "    for k, func in enumerate(univariate_funcs):\n",
    "        pkl = Path('models', model, func, 'cv_results.pkl')\n",
    "        if not pkl.exists(): # can remove when cube3_rotate gets fixed\n",
    "            continue\n",
    "        with pkl.open(\"rb\") as fh:\n",
    "            data = pickle.load(fh)\n",
    "        for i, m in enumerate(metrics):\n",
    "            arr[i, j, k] = data[m]\n",
    "\n",
    "summary = xr.DataArray(\n",
    "    arr,\n",
    "    coords={\"metric\": metrics, \"model\": model_names, \"function\": univariate_funcs},\n",
    "    dims=[\"metric\", \"model\", \"function\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd487fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric_df(metric: str) -> pd.DataFrame:\n",
    "    \"\"\"Return a model and function DataFrame for a single metric.\"\"\"\n",
    "    return summary.sel(metric=metric).to_pandas()\n",
    "\n",
    "def print_win_rate(df: pd.DataFrame, metric: str, low_wins: bool = True):\n",
    "    print(f\"\\n\\033[4m{metric}\\033[0m\")\n",
    "    print(\"  \\033[1mWin rate\\033[0m:\")\n",
    "    idxfunc = df.idxmin if low_wins else df.idxmax\n",
    "    winners = idxfunc(axis=0)\n",
    "    for model, count in winners.value_counts().items():\n",
    "        print(f\"    {model}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4ff8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rmse = metric_df('test_rmse')\n",
    "df_r    = metric_df('r_val')\n",
    "df_std  = metric_df('sigma')\n",
    "\n",
    "print_win_rate(df_r, \"Pearson's R\", low_wins=False)\n",
    "print_win_rate(df_rmse, \"Test RMSE\")\n",
    "print_win_rate(df_rmse / df_std, \"Test RMSE \\u00F7 \\u03C3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9425b09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = [\n",
    "    # 'circuit', 'cantilever_S', 'banana', 'cube3_rotate', 'steel_column',\n",
    "    'const_fn', 'const_fn3', 'const_fn15', 'cube3_rotate'\n",
    "]\n",
    "fig1 = heatmap(df_rmse.drop(cols_to_drop, axis=1), \"Test RMSE\")\n",
    "fig2 = heatmap((df_rmse/df_std).drop(cols_to_drop, axis=1), \"Test RMSE / \\u03C3\")\n",
    "fig3 = heatmap((df_rmse/df_std)[(df_rmse/df_std).drop(cols_to_drop, axis=1)>1].drop(cols_to_drop, axis=1), \"(Test RMSE / \\u03C3) > 1\")\n",
    "\n",
    "df_filtered = (df_rmse/df_std)[(df_rmse/df_std).drop(cols_to_drop, axis=1)>1].drop(cols_to_drop, axis=1)\n",
    "drop_cols = df_filtered.columns[df_filtered.isnull().all()]\n",
    "fig4 = heatmap(df_filtered.drop(drop_cols, axis=1), \"(Test RMSE / \\u03C3) > 1\")\n",
    "\n",
    "fig1.show()\n",
    "fig2.show()\n",
    "fig3.show()\n",
    "fig4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effa2eb1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "duqling_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
